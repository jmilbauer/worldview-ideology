{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worldview & Ideology Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains examples of how to perform the analysis from \"Aligning Multidimensional Worldviews and Discovering Ideological Differences\" (Milbauer et al., 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the trained embeddings, and quickly examine them to see if they make sense.\n",
    "We are using small text samples (500k tokens), so embeddings may not be very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('harris', 0.7471194267272949)\n",
      "('warren', 0.7145416736602783)\n",
      "('sanders', 0.6976892948150635)\n",
      "('bernie', 0.6945799589157104)\n",
      "('joe_biden', 0.6156193017959595)\n",
      "('kamala', 0.6149285435676575)\n",
      "('buttigieg', 0.5869153141975403)\n",
      "('candidate', 0.5764375925064087)\n",
      "('joe', 0.5390542149543762)\n",
      "('nomination', 0.5072815418243408)\n",
      "\n",
      "('harris', 0.5104426145553589)\n",
      "('creepy_joe', 0.48085713386535645)\n",
      "('joe_biden', 0.4680749475955963)\n",
      "('joe', 0.4410553574562073)\n",
      "('sniffing', 0.4272039234638214)\n",
      "('warren', 0.42609065771102905)\n",
      "('sleepy_joe', 0.4177272915840149)\n",
      "('kamala', 0.4112250804901123)\n",
      "('nominee', 0.3884388506412506)\n",
      "('hillary', 0.38795745372772217)\n"
     ]
    }
   ],
   "source": [
    "model_a = Word2Vec.load('models/politics.word2vec.model')\n",
    "model_b = Word2Vec.load('models/the_donald.word2vec.model')\n",
    "# pretrained on more data\n",
    "# model_a = Word2Vec.load('models/politics.big.model')\n",
    "# model_b = Word2Vec.load('models/the_donald.big.model')\n",
    "\n",
    "posWords = ['biden']\n",
    "negWords = []\n",
    "for x in model_a.wv.most_similar(positive=posWords, negative=negWords):\n",
    "    print(x)\n",
    "print()\n",
    "for x in model_b.wv.most_similar(positive=posWords, negative=negWords):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we find the overlapping vocabulary of the two models, and use this to construct an embedding matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_a = list(set(model_a.wv.vocab.keys()))\n",
    "vocab_b = list(set(model_b.wv.vocab.keys()))\n",
    "\n",
    "shared_vocab = set.intersection(set(vocab_a),\n",
    "                                set(vocab_b))\n",
    "shared_vocab = list(sorted(list(shared_vocab)))\n",
    "combo_vocab = set.union(set(vocab_a),\n",
    "                                set(vocab_b))\n",
    "\n",
    "w2idx = { w:i for i,w in enumerate(shared_vocab) }\n",
    "a2idx = { w:i for i,w in enumerate(vocab_a) }\n",
    "idx2b = { i:w for i,w in enumerate(vocab_b) }\n",
    "\n",
    "mtxA = np.vstack([model_a.wv[w] for w in shared_vocab])\n",
    "mtxB = np.vstack([model_b.wv[w] for w in shared_vocab])\n",
    "mtxA_ = np.vstack([model_a.wv[w] for w in vocab_a])\n",
    "mtxB_ = np.vstack([model_b.wv[w] for w in vocab_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then select only the N most common words as anchors to train our alignment. (If you're using the big model, this won't quite work because the vocabularies are different.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pickle.load(open('data/counts.pkl', 'rb'))\n",
    "n = 5000\n",
    "topN = [y for x,y in sorted([(counts[w], w) for w in w2idx if w in counts], reverse=True)][:n]\n",
    "idxs = [w2idx[w] for w in topN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorA = mtxA[idxs, :]\n",
    "anchorB = mtxB[idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use two different techniques for aligning the embeddings: SVD and CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_svd(source, target):\n",
    "    product = np.matmul(source.transpose(), target)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "    T = np.matmul(U,V)\n",
    "    return T\n",
    "\n",
    "svd = align_svd(anchorA, anchorB)\n",
    "svdA = mtxA_.dot(svd)\n",
    "svdB = mtxB_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_cca(source, target):\n",
    "    N_dims = source.shape[1]\n",
    "    cca = CCA(n_components=N_dims, max_iter=2000)\n",
    "    cca.fit(source, target)\n",
    "    return cca\n",
    "\n",
    "cca = align_cca(anchorA, anchorB)\n",
    "ccaA, ccaB = cca.transform(mtxA, mtxB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_translator(a, b, a2idx, idx2b):\n",
    "    sims = cosine_similarity(a, b)\n",
    "    most_sims = np.argsort(sims, axis=1)[:, ::-1]\n",
    "    \n",
    "    def translator(w, k=1):\n",
    "        idx = a2idx[w]\n",
    "        idxs = most_sims[idx, :k]\n",
    "        words = [idx2b[i] for i in idxs]\n",
    "        return words, sims[idx, idxs]\n",
    "    \n",
    "    return translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = build_translator(svdA, svdB, a2idx, idx2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explore three different ways of using the alignmed embeddings to explore the worldview and ideology of the two communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['democrat', 'republican', 'dem', 'democrats', 'republicans'],\n",
       " array([0.61647093, 0.58910996, 0.51375484, 0.4719858 , 0.46580005],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('democrat', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3664901664145234\n"
     ]
    }
   ],
   "source": [
    "misaligned = []\n",
    "scores = []\n",
    "\n",
    "for w in shared_vocab:\n",
    "    w_ = translator(w)[0][0]\n",
    "    s = translator(w)[1][0]\n",
    "    if w != w_:\n",
    "        misaligned.append((w, w_))\n",
    "        scores.append(s)\n",
    "        \n",
    "print(len(misaligned) / len(shared_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('performed_automatically', 'please_contact') 0.8923226\n",
      "('moderators', 'please_contact') 0.8301286\n",
      "('``', \"''\") 0.7827312\n",
      "('&', 'gt') 0.74673975\n",
      "('bot', 'performed_automatically') 0.7402881\n",
      "(';', 'gt') 0.71963507\n",
      "('though', 'but') 0.7046928\n",
      "('citizenship_question', 'census') 0.68586487\n",
      "('amp', ';') 0.68398106\n",
      "('action', 'performed_automatically') 0.6676772\n",
      "('couple', 'few') 0.6567316\n",
      "('disagree', 'agree') 0.64628285\n",
      "('dems', 'democrats') 0.6362802\n",
      "('supreme_court', 'scotus') 0.61996275\n",
      "('republican', 'democrat') 0.6085014\n",
      "('dumb', 'stupid') 0.60647255\n",
      "('26_times', 'lolita_express') 0.6013237\n",
      "('capitalism', 'communism') 0.5988106\n",
      "('jeffrey_epstein', 'epstein') 0.59700453\n",
      "('illegal_immigrants', 'illegals') 0.5922674\n"
     ]
    }
   ],
   "source": [
    "for pair, score in sorted(zip(misaligned, scores), key=lambda x:x[1], reverse=True)[:20]:\n",
    "    print(pair, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11909/11909 [00:00<00:00, 42394.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_antonyms(vocab):\n",
    "    antonyms = []\n",
    "    for w in tqdm(vocab):\n",
    "        for synset in wordnet.synsets(w):\n",
    "            for lemma in synset.lemmas():\n",
    "                if lemma.antonyms():\n",
    "                    antonyms.append((w, lemma.antonyms()[0].name()))\n",
    "    antonyms = set(antonyms)\n",
    "    return antonyms\n",
    "\n",
    "antonyms = get_antonyms(combo_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('civilian', 'military')\n",
      "('decrease', 'increase')\n",
      "('disagree', 'agree')\n",
      "('disrespect', 'respect')\n",
      "('illogical', 'logical')\n",
      "('inaccurate', 'accurate')\n",
      "('indirectly', 'directly')\n",
      "('ineffective', 'effective')\n",
      "('intolerant', 'tolerant')\n",
      "('invalid', 'valid')\n",
      "('liability', 'asset')\n",
      "('sell', 'buy')\n",
      "('sells', 'buy')\n",
      "('unreasonable', 'reasonable')\n",
      "('unwilling', 'willing')\n",
      "('weakness', 'strength')\n",
      "('west', 'east')\n"
     ]
    }
   ],
   "source": [
    "for mPair in misaligned:\n",
    "    if mPair in antonyms or (mPair[0], mPair[1]) in antonyms:\n",
    "        print(mPair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation / Conceptual Homomorphisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vocab = []\n",
    "for w in model_a.wv.vocab:\n",
    "    if w not in model_b.wv.vocab:\n",
    "        unique_vocab.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "scores = []\n",
    "for w in unique_vocab:\n",
    "    t = translator(w)\n",
    "    translations.append((w, t[0][0]))\n",
    "    scores.append(t[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('instructions_provided', 'performed_automatically') 0.71877486\n",
      "('permanent_ban', 'performed_automatically') 0.69331694\n",
      "('rule_violations', 'performed_automatically') 0.63353837\n",
      "('wishing_death/physical', 'performed_automatically') 0.594555\n",
      "('fully_participate', 'please_contact') 0.5898004\n",
      "('rulebreaking_content', 'performed_automatically') 0.5775635\n",
      "('`_youtu.be', '`') 0.55210274\n",
      "('spam_domain', 'performed_automatically') 0.5434005\n",
      "('/r/politics_within', 'performed_automatically') 0.52550036\n",
      "('troll_accusations', 'performed_automatically') 0.51061064\n",
      "('whitelisting', 'performed_automatically') 0.4963802\n",
      "('blatant_spam', 'performed_automatically') 0.48971322\n",
      "('confederate_flag', 'flag') 0.48527563\n",
      "('excluding_indians', 'persons') 0.48497242\n",
      "('site_administrators', 'link_shortener') 0.48107997\n",
      "('following_reason', 'submission') 0.48058963\n",
      "('alan_dershowitz', 'epstein') 0.48009375\n",
      "('drinking_water', 'water') 0.47866067\n",
      "('breaking_channel', 'link_shortener') 0.47774062\n",
      "('nonreputable_/', 'performed_automatically') 0.47719014\n"
     ]
    }
   ],
   "source": [
    "for pair, score in sorted(zip(translations, scores), key=lambda x:x[1], reverse=True)[:20]:\n",
    "    print(pair, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (emnlp2021)",
   "language": "python",
   "name": "emnlp2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
