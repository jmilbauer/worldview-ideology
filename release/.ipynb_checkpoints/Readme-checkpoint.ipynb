{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Worldview Toolkit\n",
    "\n",
    "The worldview toolkit is a software package that accompanies the paper: \"Discovering Multidimensional Worldview and Ideology with Embedding Alignment\" (EMNLP 2021). The software was written by Jeremiah Milbauer.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This software package allows you to replicate the experimentation and analysis from our paper. It will also enable you to perform the same kind of analysis for whatever data might be interesting to you! For the experiments contained in our paper, we optimized some of the code for our particular computing infrastructure. This package, however, does not have the same optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "What follows below is a step-by-step guide to using the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the environment\n",
    "\n",
    "First, setup your environment using the requirements.txt or requirements.yml:\n",
    "\n",
    "<code>pip install -r requirements.txt</code>\n",
    "\n",
    "<code>conda create --name wvtk --file requirements.yml</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collect Data\n",
    "\n",
    "In order to use this toolkit, you must collect text data from the communities you wish to analyze. Ideally, you will be able to collect at least 1,000,000 sentences from the community. You can draw this text from anywhere -- we used online social media communities, but some work has been done to look at how culture changes over time, treating each decade as a \"community\".\n",
    "\n",
    "Once you have collected your data, preprocess it so that the following constraints are respected:\n",
    "- Punctuation is not attached to words\n",
    "- Words are lower case\n",
    "\n",
    "You can also enforce some optional constraints, which may yield interesting results:\n",
    "- Common bigrams and trigrams have been merged into phrases\n",
    "- Words have been stemmed or lemmatized\n",
    "\n",
    "For text preprocessing, consider using NLTK. In our paper, we used NLTK to preprocess the text and form phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute Corpus Statistics\n",
    "\n",
    "An important step of this process is computing some corpus stats.\n",
    "\n",
    "You will need to compute the word frequencies for each worldview file. This will give you the shared vocabularies to use for alignment.\n",
    "\n",
    "Run the following: <code>python3 ./src/stats/counts.py ./corpus ./data/counts.json</code>\n",
    "\n",
    "You will also need to compute a general-purpose embedding to use for word clustering. You need to choose a text file that best represents \"generic\" language among your communities. You could use the union of the worldview files. That's what we did, and the method implemented by <code>multitrain.sh</code>\n",
    "\n",
    "First, build the general-purpose embedding: <code>./src/modeling/multitrain.sh ./corpus ./data/master.model</code>\n",
    "\n",
    "Then, compute the clusters: <code>python3 ./src/stats/topics.py ./data/master.model ./data/clusters.json</code>\n",
    "\n",
    "Now you're ready to move onto the model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Community Models\n",
    "\n",
    "Place your preprocessed text file, each representing one worldview, into corpus/\n",
    "\n",
    "Run: <code>./src/modeling/train.sh ./corpus ./data/models</code>\n",
    "\n",
    "You will now have trained gensim word2vec models in <code>data/models</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Align the Models\n",
    "\n",
    "Your models should be located at <code>data/models/</code>\n",
    "\n",
    "Run one of the following aligners:\n",
    "   \n",
    "- <code>python3 ./src/alignment/align.py ./data/models/ ./data/aligners/cca/ ./data/counts.json cca 1000</code>\n",
    "- <code>python3 ./src/alignment/align.py ./data/models/ ./data/aligners/svd/ ./data/counts.json svd 1000</code> (Khudabukhsh, et al.)\n",
    "- <code>python3 ./src/alignment/align.py ./data/models/ ./data/aligners/lstsq/ ./data/counts.json lstsq 1000</code>\n",
    "\n",
    "If you have a lot of embeddings, this process may take a long time while the embeddings load.\n",
    "\n",
    "Each will learn pairwise alignments between the files in the directory, and save an <code>Aligner</code> object which can be used to analyze the alignment\n",
    "\n",
    "It's also worth noting that you can experiment with the stopword strategy.\n",
    "- -1 will use all the shared words between the communities.\n",
    "- n will use the n most frequent shared words between the communities.\n",
    "- 0 will use NLTK stopwords (see Khudabukhsh, et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analyze the Alignments\n",
    "\n",
    "Now we have a number of trained aligner objects, and we can begin to use them to explore the ideological dialects of the communities they connect!\n",
    "\n",
    "Check out each of the analysis notebooks for ideas."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
